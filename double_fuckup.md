# Дотянуться до облаков или как пережить шифрование дважды

# Контекст

Это история о том, как мне пришлось внепланово быстро прокачать навыки работы с Yandex Cloud до уровня "мастер".

В одной компании К арендовали серверы в одном ЦОДе Ц и неспеша планировани миграцияю в Yandex-облака. Встречались с коллегами из Яндекса и планировани привлечь партнёров в помощь. Яндекс любезно предоставил нам грант на период тестирования.

Мы развернули сервер 1С, перевезли пару тестовых баз и плавно погружались в нюансы облака Яндекс.

# Инцидент раз

Ничто не предвещало беды (как всегда), пока мониторинг не сообщил о недоступности серверов. Быстрый анализ показал, что каналы в норме, недоступна система виртуализации VMWare в принципе.

Поддержка ЦОД сообщила об инциденте с шифрованием физических серверов и вся система виртуализации оказалась неработоспособной.

Как опытные IT-шники бекапы мы хранили отдельно и настало время переезда, только он уже не был таким плавным.

В течение нескольких дней мы экстренно поднимали сервера и инфраструктуру в Яндекс. Первоочередная цель - восстановить работу информационных систем и дать возможность бизнесу работать. За короткий срок я перелопатил половину документации Яндекса.

## Серверы 1С: Предприятие

Ещё на этапе тестирования облака обнаружилось, что некоторые базы не лезут в Postgres, поэтому Managed-версию использовать не получится. А ещё в некоторых обменах используются Com-объекты, а значит linux для нас так же закрыт.

Ну что ж, значит Windows Server 2022 + MS SQL Server + Сервер 1С:Предприятие.

Быстро разбил все информационные системы на несколько групп:

- Корпоративные системы (документооборот);
- Бухгалтерские системы (БП, ЗУП);
- Коммерческие системы (УТ, КА, и кучка бизнес-сервисов).

Поднять такую конфигурацию не проблема, делали много раз. Но оказался интересный нюанс с подготовкой образа Windows Server 2022. Инструкции как правильно готовить образ в хелпе яндекса не нашлось, а несколько попыток собрать самостоятельно не увенчались успехом. Сервер развёрнут, запущен, но недоступен.

И вот в момент когда уже начали смотреть на облака TimeWeb неожиданно возникла идея. Я поднял сервер в облаке TimeWeb, сделал несколько нужных для меня настроек и выгрузил образ в файл o_O. Яндекс без вопросов принял этот образ и с успехом развернул его.

В итоге успех! Серверы подняты, интеграции восстановлены, но… пользователи не могут зайти в систему.

## Терминальный доступ

Теринальный доступ — самый быстрый вариант организации доступа к ресурсам. Образ есть, развернул сервер, дал ему публичный IP и начали выдавать доступы самым важным участникам бизнес-процессов.

## VPN-канал

Пора предоставить доступ к облачным ресурсам на всех площадках компании К. На тот момент во всех точках стояли Mikrotik\`и. А поскольку я уже имел опыт настройки VPN-канала Mikrotik + WireGuard, то был выбран именно этот путь.

Не часто я разворачиваю WireGuard, в своей базе знаний записал только несколько команд для “домашнего” использования, но они не совсем подошли и пришлось немного “погуглить”. Оказалось, что есть несколько нюансов в поднятии канала — нужно прописать скрипты добавления правил маршрутизации на WireGuard, иначе двусторонний VPN работает не совсем как надо. Сначала на тестовом Mikrotik откатали конфигурацию, а затем раскатили её на все площадки и запустили пользователей в информационные системы 1С.

## Домен

ЦОД Ц починили, в нём восстановили файловое хранилище разнородной информации компании, подняли бекап AD. Подняли в Яндекс Облаке свою реплику домена и для всех серверов в облаке указали его как основной. В итоге в случае падения канала домен всегда есть свой. И это в будущем очень помогло.

## А что ещё?


Бекапы — есть интересные особенности в подключении серверов к системе Cloud Backup. Иногда скрипт отрабатывает не до конца и повторная инсталляция не проходит. Помогло немного ручных операций в системе с агентом и скрипт его обновления. Но если ставить агента до миграции сервера в домен, то всё проходит без проблем. Главное не забыть прописать сервисного пользователя с нужными правами и разрешить политики сетевого доступа сервера.


Квоты — оказалось, что есть квоты на хранение бекапов. Это логично, но неприятность кроется в том, что уведомления о достижении квот не приходят. Либо пока я не разобрался как их настроить. А если квоты превышены, то бекап фейлится. Поэтому нужно знать объём своих ресурсов и чётко рассчитать требуемый объём под архивы.


Безопасность — в режиме аварии конечно на безопасность слегка забили. Группы доступа как правило создавались со всем разрешённым трафиком.

# Инцидент два

Два месяца мы трудились над перестроением инфраструктуры. 

И снова шифрование… На этот раз зашифровали уже нас “изнутри” и более глобально. Зря мы уделили мало внимания Безопасности, но тут сыграло свою роль ограниченность ресурсов человеческих. Не буду углубляться в подробности - цель описать свой опыт яндекс облака.

## Восстановление инфраструктуры внутри облака

Не зря я уделил много внимания настройке бекапов на всех уровнях:

1. Сервера бекапились сервисом Cloud Backup. Возникли нюансы с квотами, о которых я писал выше и некоторых свежих бекапов не оказалось. Но есть снапшоты;
2. Снапшоты дисков. Это первое что я включал после поднятия сервера и это спасло не мало данных.
3. Бекапы баз в облако S3. На серверах был настроен агент, который готовил бекап MS SQL базы и отправлял это в хранилише S3 по токену, который имеет доступ только на upload.

Бекапы баз не пригодились, всё обошлось восстановлением Cloud Backup + снапшоты дисков.

Примерно за 24 часа вся инфраструктура в Яндекс Облако была восстановлена.

В самом восстановлении тоже есть нюансы — нужен активный агент внутри сервера. Восстанавливать в существующие сервера я не мог — ведь они зашифрованы. Поэтому разворачивал чистый сервер, подключал его к Cloud Backup, подключал все нужные диски и уже потом запускал восстановление.

## Переезд остальных ресурсов в облако

Файловый сервер


## 



За время восстановления я изучил практически всю базу знаний облака.

 


# Переезд в чистое пространство

после расследования инцидента Было принято решение объявить текущую инфраструктуру в Яндексе заражённой зоной и поднимать всё заново в чистой зоне.

Круто, что у яндекса есть такое понятие как “Каталог”. Я решил поделить всю инфраструктуру на несколько каталогов:

- Exclusion - заражённая зона с отдельной сетью;
- Infra - зона инфраструктурных сервисов (VPN-каналы, домены и другие инфраструктурные сервисы);
- Prod - очевидно продуктивная среда без прямой публикации в интернет;
- Dev — среда разработки и тут же предпродуктивная среда;
- Fire — зоне публикации ресурсов в интернет (внешний балансировщик запросов, терминальный сервис).

В зоне Infra создал виртуальную сеть и для каждой зоны своя подсеть. Важный момент - зона Exclusion отделена своей сетью и нельзя её смешивать с остальными.

## Как связать чистую и заражённую зону

Перевезти все серверы в чистую зону быстро без остановки работы бизнеса не получится, а значит нужен плавный переезд. В таком случае требуется организовать переброс трафика между зонами: чистой и заражённой. Для этого есть NGinx.

Я поставил один сервер NGInx, подключил ему 2 интерфейса из подсетей обоих зон и настроил проксирование трафика между ними. Однако при настройке работы с 1С-сервером возникли проблемы - клиент отказывался работать. На просторах интернета я нашёл информацию про модуль **ngx_stream_core_module**. Изначально этот модуль я использовал для разделения TCP и UPD трафика в одном из сервисов, но решил попробовать и для 1С. В итоге всё получилось. Трафик проксируется по всем портам, необходимым для работы 1С как надо.

Но и тут возникла подстава. В модуле не возможно обрабатывать имя сервера и организовать перенаправление трафика на нужный сервер. Ну что-ж, выход есть — поднять 4 NGinx.

Каждый NGinx - это туннель из одной зоны на конкретный сервер другой зоны. И можно сделать отдельную настройку для каждого интерфейса (важно прописывать IP интерфейса в listen).

Кстати, сервер лицензирования 1С таким способом так же работает хорошо.

## Как быстро перевезти данные

Данных на серверах достаточно много и простым копированием через интернет, S3 или что-то ещё это долго. Выход есть — перевозить диски.

Сделал снапшот диска, из него создал новый диск и через Ya CLI перенёс диск в нужную зону. Далее подключил диск к серверу, скопировал нужные данные, отключил и удалил диск.

Важно заметить, что проверку всех файлов мы выполняли ещё после первого инцидента. Таким образом переносились только файлы БД, а так же файловое хранилище информационных баз 1С (pdf, word, excel и т.д.).

# А что дальше?

## Наводим порядок в ИБ

Настраиваем группы безопасности под сервисы и серверы, разграничиваем права сервисных пользователей и защищаем все свои внешние публикации.

## Идём в сторону отказоустойчивости

Глупо побывать в облаке и не воспользоваться возможностью размещения в различных зонах доступности. В планах:

- перевезти базы на Managed Postgre SQL;
- развернуть файловое хранилище баз 1С с использованием S3 (работает — тестировал, но не в проде);
- построить кластер 1С с размещением в нескольких зонах;
- перевезти все сервисы в k8s (мультизональный).